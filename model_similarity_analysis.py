#!/usr/bin/env python3
"""
å¤§æ¨¡å‹ç›¸ä¼¼æ€§æŒ‡æ ‡è®¡ç®—å·¥å…·
å¯¹3*5ä¸ªå¤§æ¨¡å‹çš„ç»„åˆåˆ†åˆ«è®¡ç®—15ç»„æŒ‡æ ‡ï¼Œå¹¶å°†æ¯ä¸€ç±»åˆ«çš„æŒ‡æ ‡ç»“æœç»Ÿä¸€è®°å½•åœ¨ä¸€èµ·
"""

import os
import json
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from collections import defaultdict

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.family'] = ['SimHei', 'WenQuanYi Micro Hei', 'Heiti TC']
plt.rcParams['axes.unicode_minus'] = False

class ModelSimilarityAnalysis:
    """å¤§æ¨¡å‹ç›¸ä¼¼æ€§æŒ‡æ ‡è®¡ç®—åˆ†æç±»"""
    def __init__(self, output_dir="./similarity_analysis_results"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # å®šä¹‰3ä¸ªæºæ¨¡å‹å’Œ5ä¸ªç›®æ ‡æ¨¡å‹ï¼ˆå…±15ç§ç»„åˆï¼‰
        self.source_models = ['llama2-7b', 'bert-large', 'roberta-large']
        self.target_models = ['mistral-7b', 'vicuna-7b', 'guanaco-7b', 'starling-7b', 'chatgpt-3.5']
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        self.results = {
            'output_distribution': [],  # è¾“å‡ºåˆ†å¸ƒç›¸ä¼¼æ€§æŒ‡æ ‡
            'representation_space': [], # è¡¨å¾ç©ºé—´ç›¸ä¼¼æ€§æŒ‡æ ‡
            'behavior_functional': []   # è¡Œä¸º/åŠŸèƒ½ç›¸ä¼¼æ€§æŒ‡æ ‡
        }
        
    def generate_sample_prob_distributions(self, model1_name, model2_name, size=1000):
        """ç”Ÿæˆä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒæ ·æœ¬æ•°æ®"""
        # ä¸ºäº†æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒå·®å¼‚ï¼Œä½¿ç”¨ä¸åŒçš„éšæœºç§å­
        seed1 = hash(model1_name) % 1000
        seed2 = hash(model2_name) % 1000
        
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
        np.random.seed(seed1)
        # ç”Ÿæˆç¬¬ä¸€ä¸ªæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆä½¿ç”¨Dirichletåˆ†å¸ƒï¼‰
        alpha1 = np.random.uniform(0.1, 2.0, size)
        probs1 = np.exp(alpha1) / np.sum(np.exp(alpha1))
        
        # ç”Ÿæˆä¸ç¬¬ä¸€ä¸ªæ¨¡å‹ç›¸å…³çš„ç¬¬äºŒä¸ªæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒ
        np.random.seed(seed2)
        alpha2 = alpha1 * np.random.uniform(0.8, 1.2, size)
        probs2 = np.exp(alpha2) / np.sum(np.exp(alpha2))
        
        # å½’ä¸€åŒ–ç¡®ä¿æ€»å’Œä¸º1
        probs1 = probs1 / probs1.sum()
        probs2 = probs2 / probs2.sum()
        
        return probs1, probs2
    
    def generate_sample_logits(self, model1_name, model2_name, size=1000):
        """ç”Ÿæˆä¸¤ä¸ªæ¨¡å‹çš„logitsæ ·æœ¬æ•°æ®"""
        # ä¸ºäº†æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„logitså·®å¼‚ï¼Œä½¿ç”¨ä¸åŒçš„éšæœºç§å­
        seed1 = hash(model1_name) % 1000
        seed2 = hash(model2_name) % 1000
        
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
        np.random.seed(seed1)
        # ç”Ÿæˆç¬¬ä¸€ä¸ªæ¨¡å‹çš„logitsï¼ˆä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼‰
        logits1 = np.random.normal(0, 5, size)
        
        # ç”Ÿæˆä¸ç¬¬ä¸€ä¸ªæ¨¡å‹ç›¸å…³çš„ç¬¬äºŒä¸ªæ¨¡å‹çš„logits
        np.random.seed(seed2)
        correlation = np.random.uniform(0.3, 0.95)
        logits2 = correlation * logits1 + np.random.normal(0, np.sqrt(1 - correlation**2) * 5, size)
        
        return logits1, logits2
    
    def generate_sample_representations(self, model1_name, model2_name, num_samples=100, dim=768):
        """ç”Ÿæˆä¸¤ä¸ªæ¨¡å‹çš„éšè—å±‚è¡¨ç¤ºæ ·æœ¬æ•°æ®"""
        # ä¸ºäº†æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„è¡¨ç¤ºå·®å¼‚ï¼Œä½¿ç”¨ä¸åŒçš„éšæœºç§å­
        seed1 = hash(model1_name) % 1000
        seed2 = hash(model2_name) % 1000
        
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
        np.random.seed(seed1)
        # ç”Ÿæˆç¬¬ä¸€ä¸ªæ¨¡å‹çš„è¡¨ç¤ºï¼ˆä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼‰
        repr1 = np.random.normal(0, 1, (num_samples, dim))
        
        # ç”Ÿæˆä¸ç¬¬ä¸€ä¸ªæ¨¡å‹ç›¸å…³çš„ç¬¬äºŒä¸ªæ¨¡å‹çš„è¡¨ç¤º
        np.random.seed(seed2)
        # åˆ›å»ºä¸€ä¸ªéšæœºå˜æ¢çŸ©é˜µæ¥æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„è¡¨ç¤ºç©ºé—´è½¬æ¢
        transform_matrix = np.random.normal(0, 1/dim, (dim, dim))
        # æ·»åŠ ä¸€äº›å™ªå£°ä»¥æ¨¡æ‹Ÿè¡¨ç¤ºå·®å¼‚
        noise_level = np.random.uniform(0.1, 0.5)
        repr2 = np.dot(repr1, transform_matrix) + np.random.normal(0, noise_level, (num_samples, dim))
        
        return repr1, repr2
    
    def generate_sample_outputs(self, model1_name, model2_name, num_samples=100):
        """ç”Ÿæˆä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºç»“æœæ ·æœ¬æ•°æ®"""
        # ä¸ºäº†æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„è¾“å‡ºå·®å¼‚ï¼Œä½¿ç”¨ä¸åŒçš„éšæœºç§å­
        seed1 = hash(model1_name) % 1000
        seed2 = hash(model2_name) % 1000
        
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
        np.random.seed(seed1)
        # ç”Ÿæˆç¬¬ä¸€ä¸ªæ¨¡å‹çš„è¾“å‡ºï¼ˆ0è¡¨ç¤ºæ‹’ç»ï¼Œ1è¡¨ç¤ºæ¥å—ï¼‰
        outputs1 = np.random.binomial(1, 0.5, num_samples)
        
        # ç”Ÿæˆä¸ç¬¬ä¸€ä¸ªæ¨¡å‹ç›¸å…³çš„ç¬¬äºŒä¸ªæ¨¡å‹çš„è¾“å‡º
        np.random.seed(seed2)
        # åŸºç¡€ä¸€è‡´æ€§æ¦‚ç‡
        base_agreement = np.random.uniform(0.5, 0.9)
        # ç”Ÿæˆä¸outputs1ç›¸å…³çš„outputs2
        flip_prob = 1 - base_agreement
        flip_mask = np.random.binomial(1, flip_prob, num_samples)
        outputs2 = np.logical_xor(outputs1, flip_mask).astype(int)
        
        return outputs1, outputs2
    
    # ç¬¬ä¸€ç±»ï¼šè¾“å‡ºåˆ†å¸ƒç›¸ä¼¼æ€§æŒ‡æ ‡
    def compute_kl_divergence(self, p, q, epsilon=1e-10):
        """è®¡ç®—KLæ•£åº¦ï¼šD(p||q)"""
        # æ·»åŠ å°å€¼ä»¥é¿å…log(0)
        p = np.clip(p, epsilon, 1)
        q = np.clip(q, epsilon, 1)
        # å½’ä¸€åŒ–ç¡®ä¿æ€»å’Œä¸º1
        p = p / p.sum()
        q = q / q.sum()
        return np.sum(p * np.log(p / q))
    
    def compute_js_divergence(self, p, q, epsilon=1e-10):
        """è®¡ç®—Jensen-Shannonæ•£åº¦"""
        # æ·»åŠ å°å€¼ä»¥é¿å…log(0)
        p = np.clip(p, epsilon, 1)
        q = np.clip(q, epsilon, 1)
        # å½’ä¸€åŒ–ç¡®ä¿æ€»å’Œä¸º1
        p = p / p.sum()
        q = q / q.sum()
        # è®¡ç®—å¹³å‡åˆ†å¸ƒ
        m = 0.5 * (p + q)
        # è®¡ç®—JSæ•£åº¦
        return 0.5 * (self.compute_kl_divergence(p, m) + self.compute_kl_divergence(q, m))
    
    def compute_emd(self, p, q):
        """è®¡ç®—Earth Mover's Distance"""
        # ä½¿ç”¨ä¸€ç»´Wassersteinè·ç¦»ä½œä¸ºEMDçš„ç®€åŒ–ç‰ˆæœ¬
        n = len(p)
        return np.sum(np.abs(np.cumsum(p) - np.cumsum(q)))
    
    def compute_logits_cosine_similarity(self, logits1, logits2):
        """è®¡ç®—Logitsä½™å¼¦ç›¸ä¼¼åº¦"""
        # å½’ä¸€åŒ–logitså‘é‡
        logits1_norm = logits1 / np.linalg.norm(logits1)
        logits2_norm = logits2 / np.linalg.norm(logits2)
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        return np.dot(logits1_norm, logits2_norm)
    
    def compute_rbo(self, rankings1, rankings2, k=10, p=0.9):
        """è®¡ç®—Rank-Based Overlap (RBO)"""
        # åˆ›å»ºrankingsçš„å­—å…¸æ˜ å°„
        rank_dict1 = {item: idx + 1 for idx, item in enumerate(rankings1[:k])}
        rank_dict2 = {item: idx + 1 for idx, item in enumerate(rankings2[:k])}
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„é¡¹ç›®
        all_items = set(rank_dict1.keys()).union(set(rank_dict2.keys()))
        
        # è®¡ç®—RBO
        total = 0
        weight = 1
        for i in range(1, k + 1):
            # è®¡ç®—å‰iä¸ªé¡¹ç›®çš„é‡å 
            overlap = 0
            for item in all_items:
                if (item in rank_dict1 and rank_dict1[item] <= i) and (item in rank_dict2 and rank_dict2[item] <= i):
                    overlap += 1
            
            # è®¡ç®—å‰iä¸ªé¡¹ç›®çš„RBO
            total += weight * overlap / i
            weight *= p
        
        # è®¡ç®—å‰©ä½™éƒ¨åˆ†
        remaining = weight * len(set(rankings1[:k]).intersection(set(rankings2[:k]))) / k
        
        return total + remaining
    
    # ç¬¬äºŒç±»ï¼šè¡¨å¾ç©ºé—´ç›¸ä¼¼æ€§æŒ‡æ ‡
    def compute_cka(self, X, Y):
        """è®¡ç®—Centered Kernel Alignment (CKA)"""
        # ä¸­å¿ƒåŒ–
        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)
        
        # è®¡ç®—GramçŸ©é˜µ
        K = X_centered @ X_centered.T
        L = Y_centered @ Y_centered.T
        
        # å½’ä¸€åŒ–
        K_norm = np.linalg.norm(K)
        L_norm = np.linalg.norm(L)
        
        # è®¡ç®—CKA
        return np.sum(K * L.T) / (K_norm * L_norm)
    
    def compute_svcca(self, X, Y, n_components=100):
        """è®¡ç®—Singular Value Canonical Correlation Analysis (SVCCA)"""
        # ä¸­å¿ƒåŒ–
        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)
        
        # å¥‡å¼‚å€¼åˆ†è§£
        try:
            Ux, Sx, Vx = np.linalg.svd(X_centered, full_matrices=False)
            Uy, Sy, Vy = np.linalg.svd(Y_centered, full_matrices=False)
        except np.linalg.LinAlgError:
            return 0.0  # å¤„ç†SVDè®¡ç®—å¤±è´¥çš„æƒ…å†µ
        
        # é€‰æ‹©ä¸»æˆåˆ†
        n_components = min(n_components, Ux.shape[1], Uy.shape[1])
        Ux_reduced = Ux[:, :n_components]
        Uy_reduced = Uy[:, :n_components]
        
        # è®¡ç®—CCA
        C = Ux_reduced.T @ Uy_reduced
        try:
            Uc, Sc, Vc = np.linalg.svd(C, full_matrices=False)
        except np.linalg.LinAlgError:
            return 0.0  # å¤„ç†SVDè®¡ç®—å¤±è´¥çš„æƒ…å†µ
        
        # SVCCAæ˜¯ç›¸å…³ç³»æ•°çš„å¹³å‡å€¼
        return np.mean(Sc)
    
    def compute_pwcca(self, X, Y, n_components=100):
        """è®¡ç®—Weighted Singular Value Canonical Correlation Analysis (PWCCA)"""
        # ä¸­å¿ƒåŒ–
        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)
        
        # å¥‡å¼‚å€¼åˆ†è§£
        try:
            Ux, Sx, Vx = np.linalg.svd(X_centered, full_matrices=False)
            Uy, Sy, Vy = np.linalg.svd(Y_centered, full_matrices=False)
        except np.linalg.LinAlgError:
            return 0.0  # å¤„ç†SVDè®¡ç®—å¤±è´¥çš„æƒ…å†µ
        
        # é€‰æ‹©ä¸»æˆåˆ†
        n_components = min(n_components, Ux.shape[1], Uy.shape[1])
        Ux_reduced = Ux[:, :n_components]
        Uy_reduced = Uy[:, :n_components]
        
        # è®¡ç®—CCA
        C = Ux_reduced.T @ Uy_reduced
        try:
            Uc, Sc, Vc = np.linalg.svd(C, full_matrices=False)
        except np.linalg.LinAlgError:
            return 0.0  # å¤„ç†SVDè®¡ç®—å¤±è´¥çš„æƒ…å†µ
        
        # è®¡ç®—æƒé‡
        weights = Sx[:n_components] / np.sum(Sx[:n_components])
        
        # PWCCAæ˜¯åŠ æƒç›¸å…³ç³»æ•°çš„å¹³å‡å€¼
        return np.sum(weights * Sc)
    
    def compute_rsa(self, X, Y, distance_metric='correlation'):
        """è®¡ç®—Representational Similarity Analysis (RSA)"""
        # è®¡ç®—è¡¨ç¤ºç©ºé—´ä¸­çš„è·ç¦»çŸ©é˜µ
        n = X.shape[0]
        Dx = np.zeros((n, n))
        Dy = np.zeros((n, n))
        
        for i in range(n):
            for j in range(i + 1, n):
                if distance_metric == 'correlation':
                    # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
                    corr_coef = np.corrcoef(X[i], X[j])[0, 1]
                    # å¤„ç†NaNå€¼
                    if np.isnan(corr_coef):
                        corr_coef = 0.0
                    Dx[i, j] = Dx[j, i] = 1 - corr_coef
                    
                    corr_coef2 = np.corrcoef(Y[i], Y[j])[0, 1]
                    if np.isnan(corr_coef2):
                        corr_coef2 = 0.0
                    Dy[i, j] = Dy[j, i] = 1 - corr_coef2
                else:
                    Dx[i, j] = Dx[j, i] = np.linalg.norm(X[i] - X[j])
                    Dy[i, j] = Dy[j, i] = np.linalg.norm(Y[i] - Y[j])
        
        # è®¡ç®—ä¸¤ä¸ªè·ç¦»çŸ©é˜µä¹‹é—´çš„ç›¸å…³æ€§
        return np.corrcoef(Dx.flatten(), Dy.flatten())[0, 1]
    
    # ç¬¬ä¸‰ç±»ï¼šè¡Œä¸º/åŠŸèƒ½ç›¸ä¼¼æ€§æŒ‡æ ‡
    def compute_task_agreement(self, outputs1, outputs2):
        """è®¡ç®—ä»»åŠ¡ä¸€è‡´ç‡"""
        # è®¡ç®—ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºç›¸åŒçš„æ¯”ä¾‹
        return np.mean(outputs1 == outputs2)
    
    def compute_pass_at_k_agreement(self, probs1, probs2, k=1, correctness_threshold=0.5):
        """è®¡ç®—Pass@kä¸€è‡´ç‡"""
        # ç®€åŒ–ç‰ˆï¼šå‡è®¾probså·²ç»æ˜¯top-kçš„æ¦‚ç‡
        # è®¡ç®—ä¸¤ä¸ªæ¨¡å‹éƒ½æ¥å—æˆ–éƒ½æ‹’ç»çš„æ¯”ä¾‹
        pass1 = probs1 >= correctness_threshold
        pass2 = probs2 >= correctness_threshold
        return np.mean(pass1 == pass2)
    
    def compute_adversarial_transfer_rate(self, model1_name, model2_name):
        """è®¡ç®—å¯¹æŠ—è¿ç§»ç‡"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™åº”è¯¥åŸºäºçœŸå®çš„æ”»å‡»ç»“æœ
        # è¿™é‡Œæˆ‘ä»¬åŸºäºæ¨¡å‹åç§°ç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿå€¼
        # æ¨¡å‹è¶Šç›¸ä¼¼ï¼Œå¯¹æŠ—è¿ç§»ç‡è¶Šé«˜
        seed = hash(f"{model1_name}_{model2_name}") % 1000
        np.random.seed(seed)
        
        # åŸºç¡€è¿ç§»ç‡åŠ ä¸Šä¸€äº›éšæœºå˜åŒ–
        base_transfer_rate = 0.3 + 0.5 * (1 - 1/(1 + np.exp(-0.5 * seed/100)))
        transfer_rate = np.clip(base_transfer_rate + np.random.normal(0, 0.1), 0, 1)
        
        return transfer_rate
    
    def compute_semantic_similarity(self, model1_name, model2_name):
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™åº”è¯¥åŸºäºçœŸå®çš„è¾“å‡ºæ–‡æœ¬å’Œé¢„è®­ç»ƒçš„å¥å­åµŒå…¥æ¨¡å‹
        # è¿™é‡Œæˆ‘ä»¬åŸºäºæ¨¡å‹åç§°ç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿå€¼
        seed = hash(f"{model1_name}_{model2_name}_semantic") % 1000
        np.random.seed(seed)
        
        # åŸºç¡€è¯­ä¹‰ç›¸ä¼¼åº¦åŠ ä¸Šä¸€äº›éšæœºå˜åŒ–
        base_similarity = 0.4 + 0.5 * (1 - 1/(1 + np.exp(-0.5 * seed/100)))
        similarity = np.clip(base_similarity + np.random.normal(0, 0.1), 0, 1)
        
        return similarity
    
    def compute_all_metrics_for_pair(self, source_model, target_model):
        """è®¡ç®—ä¸€å¯¹æ¨¡å‹çš„æ‰€æœ‰ç›¸ä¼¼æ€§æŒ‡æ ‡"""
        print(f"\nğŸ”„ è®¡ç®—æ¨¡å‹å¯¹ {source_model} å’Œ {target_model} çš„ç›¸ä¼¼æ€§æŒ‡æ ‡")
        
        # ç”Ÿæˆæ ·æœ¬æ•°æ®
        print("  ç”Ÿæˆæ ·æœ¬æ•°æ®...")
        probs1, probs2 = self.generate_sample_prob_distributions(source_model, target_model)
        logits1, logits2 = self.generate_sample_logits(source_model, target_model)
        repr1, repr2 = self.generate_sample_representations(source_model, target_model)
        outputs1, outputs2 = self.generate_sample_outputs(source_model, target_model)
        
        # åˆ›å»ºæ’åï¼ˆåŸºäºæ¦‚ç‡ï¼‰
        rankings1 = np.argsort(-probs1)  # é™åºæ’åˆ—çš„ç´¢å¼•
        rankings2 = np.argsort(-probs2)
        
        # è®¡ç®—è¾“å‡ºåˆ†å¸ƒç›¸ä¼¼æ€§æŒ‡æ ‡
        print("  è®¡ç®—è¾“å‡ºåˆ†å¸ƒç›¸ä¼¼æ€§æŒ‡æ ‡...")
        output_metrics = {
            'source_model': source_model,
            'target_model': target_model,
            'KLæ•£åº¦': self.compute_kl_divergence(probs1, probs2),
            'JSæ•£åº¦': self.compute_js_divergence(probs1, probs2),
            'EMD': self.compute_emd(probs1, probs2),
            'Logitsä½™å¼¦ç›¸ä¼¼åº¦': self.compute_logits_cosine_similarity(logits1, logits2),
            'RBO': self.compute_rbo(rankings1, rankings2)
        }
        self.results['output_distribution'].append(output_metrics)
        
        # è®¡ç®—è¡¨å¾ç©ºé—´ç›¸ä¼¼æ€§æŒ‡æ ‡
        print("  è®¡ç®—è¡¨å¾ç©ºé—´ç›¸ä¼¼æ€§æŒ‡æ ‡...")
        representation_metrics = {
            'source_model': source_model,
            'target_model': target_model,
            'CKA': self.compute_cka(repr1, repr2),
            'SVCCA': self.compute_svcca(repr1, repr2),
            'PWCCA': self.compute_pwcca(repr1, repr2),
            'RSA': self.compute_rsa(repr1, repr2)
        }
        self.results['representation_space'].append(representation_metrics)
        
        # è®¡ç®—è¡Œä¸º/åŠŸèƒ½ç›¸ä¼¼æ€§æŒ‡æ ‡
        print("  è®¡ç®—è¡Œä¸º/åŠŸèƒ½ç›¸ä¼¼æ€§æŒ‡æ ‡...")
        behavior_metrics = {
            'source_model': source_model,
            'target_model': target_model,
            'ä»»åŠ¡ä¸€è‡´ç‡': self.compute_task_agreement(outputs1, outputs2),
            'Pass@kä¸€è‡´ç‡': self.compute_pass_at_k_agreement(probs1[:10], probs2[:10]),
            'å¯¹æŠ—è¿ç§»ç‡': self.compute_adversarial_transfer_rate(source_model, target_model),
            'è¯­ä¹‰ç›¸ä¼¼æ€§': self.compute_semantic_similarity(source_model, target_model)
        }
        self.results['behavior_functional'].append(behavior_metrics)
        
        print("  âœ… è®¡ç®—å®Œæˆ")
    
    def compute_all_metrics(self):
        """è®¡ç®—æ‰€æœ‰æ¨¡å‹å¯¹çš„ç›¸ä¼¼æ€§æŒ‡æ ‡"""
        print("ğŸ¯ å¼€å§‹è®¡ç®—æ‰€æœ‰æ¨¡å‹å¯¹çš„ç›¸ä¼¼æ€§æŒ‡æ ‡")
        print("=" * 60)
        
        total_pairs = len(self.source_models) * len(self.target_models)
        print(f"æ€»å…±æœ‰ {total_pairs} ç»„æ¨¡å‹å¯¹éœ€è¦è®¡ç®—")
        
        # å¯¹æ¯ä¸€å¯¹æ¨¡å‹è®¡ç®—æŒ‡æ ‡
        for source in self.source_models:
            for target in self.target_models:
                self.compute_all_metrics_for_pair(source, target)
        
        print("=" * 60)
        print("âœ… æ‰€æœ‰æ¨¡å‹å¯¹çš„ç›¸ä¼¼æ€§æŒ‡æ ‡è®¡ç®—å®Œæˆ")
    
    def save_results(self):
        """ä¿å­˜è®¡ç®—ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        json_path = os.path.join(self.output_dir, f"similarity_results_{timestamp}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {json_path}")
        
        # å°†æ¯ä¸€ç±»åˆ«çš„æŒ‡æ ‡ç»“æœç»Ÿä¸€è®°å½•åœ¨ä¸€èµ·
        for metric_type, metrics_list in self.results.items():
            if metrics_list:
                # åˆ›å»ºç±»åˆ«ä¸“ç”¨çš„ç»“æœæ–‡ä»¶
                type_json_path = os.path.join(self.output_dir, f"{metric_type}_results_{timestamp}.json")
                with open(type_json_path, 'w', encoding='utf-8') as f:
                    json.dump(metrics_list, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“Š {metric_type} æŒ‡æ ‡å·²ä¿å­˜åˆ°: {type_json_path}")
    
    def create_simple_visualization(self):
        """åˆ›å»ºç®€å•çš„å¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“Š å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # ä¸ºè¾“å‡ºåˆ†å¸ƒç›¸ä¼¼æ€§æŒ‡æ ‡åˆ›å»ºç®€å•çš„çƒ­åŠ›å›¾
        for metric_type, metrics_list in self.results.items():
            if not metrics_list:
                continue
                
            print(f"  åˆ›å»º {metric_type} æŒ‡æ ‡çš„ç¤ºä¾‹çƒ­åŠ›å›¾...")
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæŒ‡æ ‡åˆ›å»ºç¤ºä¾‹çƒ­åŠ›å›¾
            first_metric = None
            for key in metrics_list[0].keys():
                if key not in ['source_model', 'target_model']:
                    first_metric = key
                    break
            
            if first_metric:
                # åˆ›å»ºé€è§†è¡¨æ•°æ®
                pivot_data = np.zeros((len(self.source_models), len(self.target_models)))
                
                for idx, item in enumerate(metrics_list):
                    source_idx = self.source_models.index(item['source_model'])
                    target_idx = self.target_models.index(item['target_model'])
                    pivot_data[source_idx, target_idx] = item[first_metric]
                
                # åˆ›å»ºçƒ­åŠ›å›¾
                plt.figure(figsize=(10, 6))
                plt.imshow(pivot_data, cmap='viridis', aspect='auto')
                plt.colorbar(label=first_metric)
                plt.xticks(np.arange(len(self.target_models)), self.target_models, rotation=45)
                plt.yticks(np.arange(len(self.source_models)), self.source_models)
                plt.title(f"{metric_type.replace('_', ' ').title()} - {first_metric}")
                plt.tight_layout()
                
                # ä¿å­˜å›¾è¡¨
                chart_path = os.path.join(self.output_dir, f"{metric_type}_{first_metric}_heatmap.png")
                plt.savefig(chart_path, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"    âœ… å·²ä¿å­˜: {chart_path}")
        
        print("âœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        
        report_path = os.path.join(self.output_dir, f"similarity_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("å¤§æ¨¡å‹ç›¸ä¼¼æ€§æŒ‡æ ‡è®¡ç®—æ€»ç»“æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"è®¡ç®—æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æºæ¨¡å‹æ•°é‡: {len(self.source_models)}\n")
            f.write(f"ç›®æ ‡æ¨¡å‹æ•°é‡: {len(self.target_models)}\n")
            f.write(f"æ¨¡å‹å¯¹æ€»æ•°: {len(self.source_models) * len(self.target_models)}\n\n")
            
            f.write("æºæ¨¡å‹åˆ—è¡¨:\n")
            for model in self.source_models:
                f.write(f"  - {model}\n")
            f.write("\n")
            
            f.write("ç›®æ ‡æ¨¡å‹åˆ—è¡¨:\n")
            for model in self.target_models:
                f.write(f"  - {model}\n")
            f.write("\n")
            
            # ä¸ºæ¯ç±»æŒ‡æ ‡ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            for metric_type, metrics_list in self.results.items():
                if not metrics_list:
                    continue
                    
                f.write(f"\n{metric_type.replace('_', ' ').title()} æŒ‡æ ‡ç»Ÿè®¡:\n")
                f.write("-" * 60 + "\n")
                
                # è·å–æŒ‡æ ‡åç§°
                metric_names = [k for k in metrics_list[0].keys() if k not in ['source_model', 'target_model']]
                
                for metric in metric_names:
                    values = [item[metric] for item in metrics_list]
                    f.write(f"{metric}:\n")
                    f.write(f"  å¹³å‡å€¼: {np.mean(values):.4f}\n")
                    f.write(f"  æ ‡å‡†å·®: {np.std(values):.4f}\n")
                    f.write(f"  æœ€å¤§å€¼: {np.max(values):.4f}\n")
                    f.write(f"  æœ€å°å€¼: {np.min(values):.4f}\n\n")
        
        print(f"ğŸ’¾ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºç›¸ä¼¼æ€§æŒ‡æ ‡è®¡ç®—å™¨
    metrics_calculator = ModelSimilarityAnalysis()
    
    # è®¡ç®—æ‰€æœ‰æ¨¡å‹å¯¹çš„ç›¸ä¼¼æ€§æŒ‡æ ‡
    metrics_calculator.compute_all_metrics()
    
    # ä¿å­˜ç»“æœï¼ˆåŒ…æ‹¬æŒ‰ç±»åˆ«ç»Ÿä¸€è®°å½•ï¼‰
    metrics_calculator.save_results()
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    metrics_calculator.create_simple_visualization()
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    metrics_calculator.generate_summary_report()
    
    print("\nğŸ‰ å¤§æ¨¡å‹ç›¸ä¼¼æ€§æŒ‡æ ‡è®¡ç®—ä»»åŠ¡å·²å®Œæˆ!")


if __name__ == "__main__":
    main()